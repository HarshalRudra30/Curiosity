<div align="center">

# Curiosity
### Implementing Neural Networks from scratch
</div>

<div align=center>

[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://forthebadge.com)
<img src="https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white" width=130 height=37></img>
</div>

## What are Neural Networks?
<p>“Artificial” neural networks are inspired by the organic brain, translated to the computer. It’s not
a perfect comparison, but there are neurons, activations, and lots of interconnectivity, even if the
underlying processes are quite different </br></p>
<p>A single neuron by itself is relatively useless, but, when combined with hundreds or thousands
(or many more) of other neurons, the interconnectivity produces relationships and results that
frequently outperform any other machine learning methods </br></p>

<div align="center">
<img src="https://miro.medium.com/max/1313/1*HD3Sv4r03h3iEMHw2nSFug.gif"></img>
</div>


## Why implenting from scratch?
Even though various libraries such as Pytorch, Tensorflow are available one cannot understand how neural networks actually work at core level by using the libraries.</br>
Understanding how things work at core level helps one to tune in fine hyperparameters and even solve various errors


## Topics covered:
### 1. [Activation functions](https://github.com/HarshalRudra30/Neural-Networks-From-Scratch/tree/master/Activation_Functions):
  <ul>
  <li>Linear Activation</li>
  <li>ReLU Activation</li>
  <li>Sigmoid Activation</li>
  <li>Softmax Activation</li>
  </ul>
  
  
### 2.[Loss Functions](https://github.com/HarshalRudra30/Neural-Networks-From-Scratch/tree/master/Loss):
<ul>
  <li>Binary Cross Entropy Loss</li>
  <li>Categorical Cross Entropy Loss</li>
  <li>Mean Absolute Error Loss</li>
  <li>Mean Squared Loss</li>
  </ul>
  
### 3.[Optimizers](https://github.com/HarshalRudra30/Neural-Networks-From-Scratch/tree/master/Optimizers):
<ul>
  <li>Stochastic Gradient Descent Optimizer (SGD)</li>
  <li>Adagrad Optimizer</li>
  <li>Adam Optimizer</li>
  <li>Root Mean Squared Propagation Optimizer (RMSprop)</li>
  </ul>
  
  ## Final Model
  The [complete model](https://github.com/HarshalRudra30/Neural-Networks-From-Scratch/tree/master/Final_DNN) is tested on Fashion MNIST Dataset</br>
  After spending some time and finding the best hyperparameters, an accuracy of <strong>90%</strong> was achieved.
  
  ## Support
  ⭐ Please Star and share the repository. Thanks! ❤️
